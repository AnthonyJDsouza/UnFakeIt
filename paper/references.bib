@misc{abnarQuantifyingAttentionFlow2020,
  title = {Quantifying {{Attention Flow}} in {{Transformers}}},
  author = {Abnar, Samira and Zuidema, Willem},
  year = {2020},
  month = may,
  number = {arXiv:2005.00928},
  eprint = {2005.00928},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.00928},
  urldate = {2025-06-15},
  abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/tororo.in/Zotero/storage/MYV45C4X/Abnar and Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf;/home/tororo.in/Zotero/storage/5KBH84CX/2005.html}
}

@inproceedings{aghasanliInterpretableThroughPrototypesDeepfakeDetection2023,
  title = {Interpretable-{{Through-Prototypes Deepfake Detection}} for {{Diffusion Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Aghasanli, Agil and Kangin, Dmitry and Angelov, Plamen},
  year = {2023},
  pages = {467--474},
  urldate = {2025-06-15},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/QBVC9FL7/Aghasanli et al. - 2023 - Interpretable-Through-Prototypes Deepfake Detection for Diffusion Models.pdf}
}

@inproceedings{ciamarraDeepfakeDetectionExploiting2024,
  title = {Deepfake {{Detection}} by {{Exploiting Surface Anomalies}}: {{The SurFake Approach}}},
  shorttitle = {Deepfake {{Detection}} by {{Exploiting Surface Anomalies}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Ciamarra, Andrea and Caldelli, Roberto and Becattini, Federico and Seidenari, Lorenzo and Del Bimbo, Alberto},
  year = {2024},
  pages = {1024--1033},
  urldate = {2025-06-15},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/BE59BAQ8/Ciamarra et al. - 2024 - Deepfake Detection by Exploiting Surface Anomalies The SurFake Approach.pdf}
}

@inproceedings{cozzolinoRaisingBarAIgenerated2024,
  title = {Raising the {{Bar}} of {{AI-generated Image Detection}} with {{CLIP}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cozzolino, Davide and Poggi, Giovanni and Corvi, Riccardo and Nie{\ss}ner, Matthias and Verdoliva, Luisa},
  year = {2024},
  pages = {4356--4366},
  urldate = {2025-06-15},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/7UMD4G6Q/Cozzolino et al. - 2024 - Raising the Bar of AI-generated Image Detection with CLIP.pdf}
}

@inproceedings{cozzolinoZeroShotDetectionAIGenerated2025,
  title = {Zero-{{Shot Detection}} of~{{AI-Generated Images}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2024},
  author = {Cozzolino, Davide and Poggi, Giovanni and Nie{\ss}ner, Matthias and Verdoliva, Luisa},
  editor = {Leonardis, Ale{\v s} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l},
  year = {2025},
  pages = {54--72},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-72649-1_4},
  abstract = {Detecting AI-generated images has become an extraordinarily difficult challenge as new generative architectures emerge on a daily basis with more and more capabilities and unprecedented realism. New versions of many commercial tools, such as DALL\$\${\textbackslash}cdot \$\${$\cdot$}E, Midjourney, and Stable Diffusion, have been released recently, and it is impractical to continually update and retrain supervised forensic detectors to handle such a large variety of models. To address this challenge, we propose a zero-shot entropy-based detector (ZED) that neither needs AI-generated training data nor relies on knowledge of generative architectures to artificially synthesize their artifacts. Inspired by recent works on machine-generated text detection, our idea is to measure how surprising the image under analysis is compared to a model of real images. To this end, we rely on a lossless image encoder that estimates the probability distribution of each pixel given its context. To ensure computational efficiency, the encoder has a multi-resolution architecture and contexts comprise mostly pixels of the lower-resolution version of the image. Since only real images are needed to learn the model, the detector is independent of generator architectures and synthetic training data. Using a single discriminative feature, the proposed detector achieves state-of-the-art performance. On a wide variety of generative models it achieves an average improvement of more than 3\% over the SoTA in terms of accuracy. Code is available at https://grip-unina.github.io/ZED/.},
  isbn = {978-3-031-72649-1},
  langid = {english}
}

@inproceedings{cozzolinoZeroShotDetectionAIGenerated2025a,
  title = {Zero-{{Shot Detection}} of~{{AI-Generated Images}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2024},
  author = {Cozzolino, Davide and Poggi, Giovanni and Nie{\ss}ner, Matthias and Verdoliva, Luisa},
  editor = {Leonardis, Ale{\v s} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l},
  year = {2025},
  pages = {54--72},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-72649-1_4},
  abstract = {Detecting AI-generated images has become an extraordinarily difficult challenge as new generative architectures emerge on a daily basis with more and more capabilities and unprecedented realism. New versions of many commercial tools, such as DALL\$\${\textbackslash}cdot \$\${$\cdot$}E, Midjourney, and Stable Diffusion, have been released recently, and it is impractical to continually update and retrain supervised forensic detectors to handle such a large variety of models. To address this challenge, we propose a zero-shot entropy-based detector (ZED) that neither needs AI-generated training data nor relies on knowledge of generative architectures to artificially synthesize their artifacts. Inspired by recent works on machine-generated text detection, our idea is to measure how surprising the image under analysis is compared to a model of real images. To this end, we rely on a lossless image encoder that estimates the probability distribution of each pixel given its context. To ensure computational efficiency, the encoder has a multi-resolution architecture and contexts comprise mostly pixels of the lower-resolution version of the image. Since only real images are needed to learn the model, the detector is independent of generator architectures and synthetic training data. Using a single discriminative feature, the proposed detector achieves state-of-the-art performance. On a wide variety of generative models it achieves an average improvement of more than 3\% over the SoTA in terms of accuracy. Code is available at https://grip-unina.github.io/ZED/.},
  isbn = {978-3-031-72649-1},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/M5ZS2GX2/Cozzolino et al. - 2025 - Zero-Shot Detection ofÂ AI-Generated Images.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  urldate = {2025-06-15},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  file = {/home/tororo.in/Zotero/storage/DNU4AV83/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf}
}

@inproceedings{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  year = {2021},
  volume = {34},
  pages = {8780--8794},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-16},
  file = {/home/tororo.in/Zotero/storage/WTVX9S6A/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf}
}

@misc{dolhanskyDeepFakeDetectionChallenge2020,
  title = {The {{DeepFake Detection Challenge}} ({{DFDC}}) {{Dataset}}},
  author = {Dolhansky, Brian and Bitton, Joanna and Pflaum, Ben and Lu, Jikuo and Howes, Russ and Wang, Menglin and Ferrer, Cristian Canton},
  year = {2020},
  month = oct,
  number = {arXiv:2006.07397},
  eprint = {2006.07397},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.07397},
  urldate = {2025-06-16},
  abstract = {Deepfakes are a recent off-the-shelf manipulation technique that allows anyone to swap two identities in a single video. In addition to Deepfakes, a variety of GAN-based face swapping methods have also been published with accompanying code. To counter this emerging threat, we have constructed an extremely large face swap video dataset to enable the training of detection models, and organized the accompanying DeepFake Detection Challenge (DFDC) Kaggle competition. Importantly, all recorded subjects agreed to participate in and have their likenesses modified during the construction of the face-swapped dataset. The DFDC dataset is by far the largest currently and publicly available face swap video dataset, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. In addition to describing the methods used to construct the dataset, we provide a detailed analysis of the top submissions from the Kaggle contest. We show although Deepfake detection is extremely difficult and still an unsolved problem, a Deepfake detection model trained only on the DFDC can generalize to real "in-the-wild" Deepfake videos, and such a model can be a valuable analysis tool when analyzing potentially Deepfaked videos. Training, validation and testing corpuses can be downloaded from https://ai.facebook.com/datasets/dfdc.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/tororo.in/Zotero/storage/PUSVBIHM/Dolhansky et al. - 2020 - The DeepFake Detection Challenge (DFDC) Dataset.pdf;/home/tororo.in/Zotero/storage/G5DYUIQL/2006.html}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-06-15},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/tororo.in/Zotero/storage/W8TXT96B/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/home/tororo.in/Zotero/storage/5DI2TV9R/2010.html}
}

@article{feiExposingAIgeneratedVideos2021,
  title = {Exposing {{AI-generated}} Videos with Motion Magnification},
  author = {Fei, Jianwei and Xia, Zhihua and Yu, Peipeng and Xiao, Fengjun},
  year = {2021},
  month = aug,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {20},
  pages = {30789--30802},
  issn = {1573-7721},
  doi = {10.1007/s11042-020-09147-3},
  urldate = {2025-06-15},
  abstract = {Recent progress of artificial intelligence makes it easier to edit facial movements in videos or create face substitutions, bringing new challenges to anti-fake-faces techniques. Although multimedia forensics provides many detection algorithms from a traditional point of view, it is increasingly hard to discriminate the fake videos from real ones while they become more sophisticated and plausible with updated forgery technologies. In this paper, we introduce a motion discrepancy based method that can effectively differentiate AI-generated fake videos from real ones. The amplitude of face motions in videos is first magnified, and fake videos will show more serious distortion or flicker than the pristine videos. We pre-trained a deep CNN on frames extracted from the training videos and the output vectors of the frame sequences are used as input of an LSTM at secondary training stage. Our approach is evaluated over a large fake video dataset Faceforensics++ produced by various advanced generation technologies, it shows superior performance contrasted to existing pixel-based fake video forensics approaches.},
  langid = {english},
  keywords = {Biometrics,Computer Vision,Deep learning,DeepFakes detection,Fake videos,Forensic Science,Image Processing,Motion Detection,Motion magnification,Motion Perception},
  file = {/home/tororo.in/Zotero/storage/AD3YH6ZP/Fei et al. - 2021 - Exposing AI-generated videos with motion magnification.pdf}
}

@article{heidariDeepfakeDetectionUsing2024,
  title = {Deepfake Detection Using Deep Learning Methods: {{A}} Systematic and Comprehensive Review},
  shorttitle = {Deepfake Detection Using Deep Learning Methods},
  author = {Heidari, Arash and Jafari Navimipour, Nima and Dag, Hasan and Unal, Mehmet},
  year = {2024},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {14},
  number = {2},
  pages = {e1520},
  issn = {1942-4795},
  doi = {10.1002/widm.1520},
  urldate = {2025-06-15},
  abstract = {Deep Learning (DL) has been effectively utilized in various complicated challenges in healthcare, industry, and academia for various purposes, including thyroid diagnosis, lung nodule recognition, computer vision, large data analytics, and human-level control. Nevertheless, developments in digital technology have been used to produce software that poses a threat to democracy, national security, and confidentiality. Deepfake is one of those DL-powered apps that has lately surfaced. So, deepfake systems can create fake images primarily by replacement of scenes or images, movies, and sounds that humans cannot tell apart from real ones. Various technologies have brought the capacity to change a synthetic speech, image, or video to our fingers. Furthermore, video and image frauds are now so convincing that it is hard to distinguish between false and authentic content with the naked eye. It might result in various issues and ranging from deceiving public opinion to using doctored evidence in a court. For such considerations, it is critical to have technologies that can assist us in discerning reality. This study gives a complete assessment of the literature on deepfake detection strategies using DL-based algorithms. We categorize deepfake detection methods in this work based on their applications, which include video detection, image detection, audio detection, and hybrid multimedia detection. The objective of this paper is to give the reader a better knowledge of (1) how deepfakes are generated and identified, (2) the latest developments and breakthroughs in this realm, (3) weaknesses of existing security methods, and (4) areas requiring more investigation and consideration. The results suggest that the Conventional Neural Networks (CNN) methodology is the most often employed DL method in publications. According to research, the majority of the articles are on the subject of video deepfake detection. The majority of the articles focused on enhancing only one parameter, with the accuracy parameter receiving the most attention. This article is categorized under: Technologies {$>$} Machine Learning Algorithmic Development {$>$} Multimedia Application Areas {$>$} Science and Technology},
  copyright = {{\copyright} 2023 Kadir Has University. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {deep learning,deepfake,detection,neural networks,review},
  file = {/home/tororo.in/Zotero/storage/42UCK2IB/Heidari et al. - 2024 - Deepfake detection using deep learning methods A systematic and comprehensive review.pdf;/home/tororo.in/Zotero/storage/UZTNFT43/widm.html}
}

@inproceedings{hendrycks*AugMixSimpleData2019,
  title = {{{AugMix}}: {{A Simple Data Processing Method}} to {{Improve Robustness}} and {{Uncertainty}}},
  shorttitle = {{{AugMix}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hendrycks*, Dan and Mu*, Norman and Cubuk, Ekin Dogus and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  year = {2019},
  month = sep,
  urldate = {2025-06-16},
  abstract = {Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/KW47TWGK/Hendrycks et al. - 2019 - AugMix A Simple Data Processing Method to Improve Robustness and Uncertainty.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Comput.},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2025-06-15},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-16},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {/home/tororo.in/Zotero/storage/PLSI4HTT/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@article{hongWildFakeLargeScaleHierarchical2025,
  title = {{{WildFake}}: {{A Large-Scale}} and {{Hierarchical Dataset}} for {{AI-Generated Images Detection}}},
  shorttitle = {{{WildFake}}},
  author = {Hong, Yan and Feng, Jianming and Chen, Haoxing and Lan, Jun and Zhu, Huijia and Wang, Weiqiang and Zhang, Jianfu},
  year = {2025},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {39},
  number = {4},
  pages = {3500--3508},
  issn = {2374-3468},
  doi = {10.1609/aaai.v39i4.32363},
  urldate = {2025-06-16},
  abstract = {The development of text-to-image generative models has enabled the creation of images so realistic that distinguishing between AI-generated images and real photos is becoming a challenge. This progress offers new possibilities but also raises concerns over privacy, authenticity, and security. Detecting AI-generated images is crucial to prevent misuse. To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake. This dataset features cutting-edge image generators, a wide variety of generator categories, and generators for various applications, organized in a hierarchical framework. WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles. Its design significantly improves the effectiveness of detection algorithms, making it a valuable resource for enhancing AI-generated image detection in practical applications. Our evaluations offer insights into the performance of generative models at various levels, showcasing WildFake's unique hierarchical structure's benefits.},
  copyright = {Copyright (c) 2025 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/2QTBTDK5/Hong et al. - 2025 - WildFake A Large-Scale and Hierarchical Dataset for AI-Generated Images Detection.pdf}
}

@inproceedings{huangSIDASocialMedia2025,
  title = {{{SIDA}}: {{Social Media Image Deepfake Detection}}, {{Localization}} and {{Explanation}} with {{Large Multimodal Model}}},
  shorttitle = {{{SIDA}}},
  booktitle = {Proceedings of the {{Computer Vision}} and {{Pattern Recognition Conference}}},
  author = {Huang, Zhenglin and Hu, Jinwei and Li, Xiangtai and He, Yiwei and Zhao, Xingyu and Peng, Bei and Wu, Baoyuan and Huang, Xiaowei and Cheng, Guangliang},
  year = {2025},
  pages = {28831--28841},
  urldate = {2025-06-15},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/7XJHZ537/Huang et al. - 2025 - SIDA Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Mode.pdf}
}

@article{kaurDeepfakeVideoDetection2024,
  title = {Deepfake Video Detection: Challenges and Opportunities},
  shorttitle = {Deepfake Video Detection},
  author = {Kaur, Achhardeep and Noori Hoshyar, Azadeh and Saikrishna, Vidya and Firmin, Selena and Xia, Feng},
  year = {2024},
  month = may,
  journal = {Artificial Intelligence Review},
  volume = {57},
  number = {6},
  pages = {159},
  issn = {1573-7462},
  doi = {10.1007/s10462-024-10810-6},
  urldate = {2025-06-15},
  abstract = {Deepfake videos are a growing social issue. These videos are manipulated by artificial intelligence (AI) techniques (especially deep learning), an emerging societal issue. Malicious individuals misuse deepfake technologies to spread false information, such as fake images, videos, and audio. The development of convincing fake content threatens politics, security, and privacy. The majority of deepfake video detection methods are data-driven. This survey paper aims to thoroughly analyse deepfake video generation and detection. The paper's main contribution is the classification of the many challenges encountered while detecting deepfake videos. The paper discusses data challenges such as unbalanced datasets and inadequate labelled training data. Training challenges include the need for many computational resources. It also addresses reliability challenges, including overconfidence in detection methods and emerging manipulation approaches. The research emphasises the dominance of deep learning-based methods in detecting deepfakes despite their computational efficiency and generalisation limitations. However, it also acknowledges the drawbacks of these approaches, such as their limited computing efficiency and generalisation. The research also critically evaluates deepfake datasets, emphasising the necessity for good-quality datasets to improve detection methods. The study also indicates major research gaps, guiding future deepfake detection research. This entails developing robust models for real-time detection.},
  langid = {english},
  keywords = {Artificial Intelligence,Computational time,Computer Vision,Deep learning,Deepfake detection,Digital Religion,Digital Sociology,Efficiency,Fake video,Generalisation,Internetpsychology,Machine Learning},
  file = {/home/tororo.in/Zotero/storage/2R2JTPN2/Kaur et al. - 2024 - Deepfake video detection challenges and opportunities.pdf}
}

@inproceedings{kunduUniversalSyntheticVideo2025,
  title = {Towards a {{Universal Synthetic Video Detector}}: {{From Face}} or {{Background Manipulations}} to {{Fully AI-Generated Content}}},
  shorttitle = {Towards a {{Universal Synthetic Video Detector}}},
  booktitle = {Proceedings of the {{Computer Vision}} and {{Pattern Recognition Conference}}},
  author = {Kundu, Rohit and Xiong, Hao and Mohanty, Vishal and Balachandran, Athula and {Roy-Chowdhury}, Amit K.},
  year = {2025},
  pages = {28050--28060},
  urldate = {2025-06-15},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/GX5GZEMK/Kundu et al. - 2025 - Towards a Universal Synthetic Video Detector From Face or Background Manipulations to Fully AI-Gene.pdf}
}

@misc{laiLISAReasoningSegmentation2024,
  title = {{{LISA}}: {{Reasoning Segmentation}} via {{Large Language Model}}},
  shorttitle = {{{LISA}}},
  author = {Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  year = {2024},
  month = may,
  number = {arXiv:2308.00692},
  eprint = {2308.00692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.00692},
  urldate = {2025-06-15},
  abstract = {Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a {$<$}SEG{$>$} token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tororo.in/Zotero/storage/Q9FGLAAH/Lai et al. - 2024 - LISA Reasoning Segmentation via Large Language Model.pdf;/home/tororo.in/Zotero/storage/4A9H5M3K/2308.html}
}

@misc{liBLIPBootstrappingLanguageImage2022,
  title = {{{BLIP}}: {{Bootstrapping Language-Image Pre-training}} for {{Unified Vision-Language Understanding}} and {{Generation}}},
  shorttitle = {{{BLIP}}},
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  year = {2022},
  month = feb,
  number = {arXiv:2201.12086},
  eprint = {2201.12086},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.12086},
  urldate = {2025-06-15},
  abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tororo.in/Zotero/storage/W6NL7JQY/Li et al. - 2022 - BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Genera.pdf;/home/tororo.in/Zotero/storage/PVY5HWA9/2201.html}
}

@inproceedings{liCelebDFLargeScaleChallenging2020,
  title = {Celeb-{{DF}}: {{A Large-Scale Challenging Dataset}} for {{DeepFake Forensics}}},
  shorttitle = {Celeb-{{DF}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Yuezun and Yang, Xin and Sun, Pu and Qi, Honggang and Lyu, Siwei},
  year = {2020},
  month = jun,
  pages = {3204--3213},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.00327},
  urldate = {2025-06-16},
  abstract = {AI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. The need to develop and evaluate DeepFake detection algorithms calls for datasets of DeepFake videos. However, current DeepFake datasets suffer from low visual quality and do not resemble DeepFake videos circulated on the Internet. We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5,639 high-quality DeepFake videos of celebrities generated using improved synthesis process. We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF.},
  keywords = {Decoding,Detection algorithms,Image color analysis,Training,Videos,Visualization,YouTube},
  file = {/home/tororo.in/Zotero/storage/T2PID53C/Li et al. - 2020 - Celeb-DF A Large-Scale Challenging Dataset for DeepFake Forensics.pdf;/home/tororo.in/Zotero/storage/YCQVDBW9/9156368.html}
}

@misc{liExposingDeepFakeVideos2019,
  title = {Exposing {{DeepFake Videos By Detecting Face Warping Artifacts}}},
  author = {Li, Yuezun and Lyu, Siwei},
  year = {2019},
  month = may,
  number = {arXiv:1811.00656},
  eprint = {1811.00656},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.00656},
  urldate = {2025-06-16},
  abstract = {In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as \{{\textbackslash}em DeepFake\} videos hereafter) from real videos. Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video. Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs). Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classifier, our method does not need DeepFake generated images as negative training examples since we target the artifacts in affine face warping as the distinctive feature to distinguish real and fake images. The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example. Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others. Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tororo.in/Zotero/storage/WRUX58TI/Li and Lyu - 2019 - Exposing DeepFake Videos By Detecting Face Warping Artifacts.pdf;/home/tororo.in/Zotero/storage/K38L4PLE/1811.html}
}

@misc{lipmanFlowMatchingGenerative2023,
  title = {Flow {{Matching}} for {{Generative Modeling}}},
  author = {Lipman, Yaron and Chen, Ricky T. Q. and {Ben-Hamu}, Heli and Nickel, Maximilian and Le, Matt},
  year = {2023},
  month = feb,
  number = {arXiv:2210.02747},
  eprint = {2210.02747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.02747},
  urldate = {2025-06-16},
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tororo.in/Zotero/storage/J25X3C8U/Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf;/home/tororo.in/Zotero/storage/CR2QLN26/2210.html}
}

@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = jun,
  pages = {10674--10685},
  publisher = {IEEE Computer Society},
  doi = {10.1109/CVPR52688.2022.01042},
  urldate = {2025-06-16},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/3JTF28UN/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf}
}

@misc{rosslerFaceForensicsLargescaleVideo2018,
  title = {{{FaceForensics}}: {{A Large-scale Video Dataset}} for {{Forgery Detection}} in {{Human Faces}}},
  shorttitle = {{{FaceForensics}}},
  author = {R{\"o}ssler, Andreas and Cozzolino, Davide and Verdoliva, Luisa and Riess, Christian and Thies, Justus and Nie{\ss}ner, Matthias},
  year = {2018},
  month = mar,
  number = {arXiv:1803.09179},
  eprint = {1803.09179},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.09179},
  urldate = {2025-06-16},
  abstract = {With recent advances in computer vision and graphics, it is now possible to generate videos with extremely realistic synthetic faces, even in real time. Countless applications are possible, some of which raise a legitimate alarm, calling for reliable detectors of fake videos. In fact, distinguishing between original and manipulated video can be a challenge for humans and computers alike, especially when the videos are compressed or have low resolution, as it often happens on social networks. Research on the detection of face manipulations has been seriously hampered by the lack of adequate datasets. To this end, we introduce a novel face manipulation dataset of about half a million edited images (from over 1000 videos). The manipulations have been generated with a state-of-the-art face editing approach. It exceeds all existing video manipulation datasets by at least an order of magnitude. Using our new dataset, we introduce benchmarks for classical image forensic tasks, including classification and segmentation, considering videos compressed at various quality levels. In addition, we introduce a benchmark evaluation for creating indistinguishable forgeries with known ground truth; for instance with generative refinement models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tororo.in/Zotero/storage/ZAGKH7N6/RÃ¶ssler et al. - 2018 - FaceForensics A Large-scale Video Dataset for Forgery Detection in Human Faces.pdf;/home/tororo.in/Zotero/storage/9T4APQU5/1803.html}
}

@inproceedings{rosslerFaceForensicsLearningDetect2019,
  title = {{{FaceForensics}}++: {{Learning}} to {{Detect Manipulated Facial Images}}},
  shorttitle = {{{FaceForensics}}++},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Rossler, Andreas and Cozzolino, Davide and Verdoliva, Luisa and Riess, Christian and Thies, Justus and Niessner, Matthias},
  year = {2019},
  pages = {1--11},
  urldate = {2025-06-16},
  file = {/home/tororo.in/Zotero/storage/Y6DRL79Q/Rossler et al. - 2019 - FaceForensics++ Learning to Detect Manipulated Facial Images.pdf}
}

@inproceedings{ruizDreamBoothFineTuning2023,
  title = {{{DreamBooth}}: {{Fine Tuning Text-to-Image Diffusion Models}} for {{Subject-Driven Generation}}},
  shorttitle = {{{DreamBooth}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  year = {2023},
  pages = {22500--22510},
  urldate = {2025-06-16},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/PP4VNN4T/Ruiz et al. - 2023 - DreamBooth Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.pdf}
}

@misc{seferbekovDeepfakeDetectionChallenge,
  title = {Deepfake {{Detection Challenge}}},
  author = {Seferbekov, Selim},
  urldate = {2025-06-16},
  abstract = {Identify videos with facial or voice manipulations},
  howpublished = {https://kaggle.com/deepfake-detection-challenge},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/KUBCWI7T/145721.html}
}

@misc{szegedyRethinkingInceptionArchitecture2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year = {2015},
  month = dec,
  number = {arXiv:1512.00567},
  eprint = {1512.00567},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.00567},
  urldate = {2025-06-15},
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tororo.in/Zotero/storage/WAJYDUBU/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision.pdf;/home/tororo.in/Zotero/storage/WMTHD8HU/1512.html}
}

@inproceedings{xianUprightNetGeometryAwareCamera2019,
  title = {{{UprightNet}}: {{Geometry-Aware Camera Orientation Estimation From Single Images}}},
  shorttitle = {{{UprightNet}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Xian, Wenqi and Li, Zhengqi and Fisher, Matthew and Eisenmann, Jonathan and Shechtman, Eli and Snavely, Noah},
  year = {2019},
  pages = {9974--9983},
  urldate = {2025-06-15},
  file = {/home/tororo.in/Zotero/storage/VKJEPSIM/Xian et al. - 2019 - UprightNet Geometry-Aware Camera Orientation Estimation From Single Images.pdf}
}

@misc{yanSanityCheckAIgenerated2025,
  title = {A {{Sanity Check}} for {{AI-generated Image Detection}}},
  author = {Yan, Shilin and Li, Ouxiang and Cai, Jiayin and Hao, Yanbin and Jiang, Xiaolong and Hu, Yao and Xie, Weidi},
  year = {2025},
  month = feb,
  number = {arXiv:2406.19435},
  eprint = {2406.19435},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.19435},
  urldate = {2025-06-15},
  abstract = {With the rapid development of generative models, discerning AI-generated content has evoked increasing attention from both industry and academia. In this paper, we conduct a sanity check on "whether the task of AI-generated image detection has been solved". To start with, we present Chameleon dataset, consisting AIgenerated images that are genuinely challenging for human perception. To quantify the generalization of existing methods, we evaluate 9 off-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis, almost all models classify AI-generated images as real ones. Later, we propose AIDE (AI-generated Image DEtector with Hybrid Features), which leverages multiple experts to simultaneously extract visual artifacts and noise patterns. Specifically, to capture the high-level semantics, we utilize CLIP to compute the visual embedding. This effectively enables the model to discern AI-generated images based on semantics or contextual information; Secondly, we select the highest frequency patches and the lowest frequency patches in the image, and compute the low-level patchwise features, aiming to detect AI-generated images by low-level artifacts, for example, noise pattern, anti-aliasing, etc. While evaluating on existing benchmarks, for example, AIGCDetectBenchmark and GenImage, AIDE achieves +3.5\% and +4.6\% improvements to state-of-the-art methods, and on our proposed challenging Chameleon benchmarks, it also achieves the promising results, despite this problem for detecting AI-generated images is far from being solved.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tororo.in/Zotero/storage/JYY2S29F/Yan et al. - 2025 - A Sanity Check for AI-generated Image Detection.pdf;/home/tororo.in/Zotero/storage/IYJIGJZ7/2406.html}
}

@misc{yunCutMixRegularizationStrategy2019,
  title = {{{CutMix}}: {{Regularization Strategy}} to {{Train Strong Classifiers}} with {{Localizable Features}}},
  shorttitle = {{{CutMix}}},
  author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  year = {2019},
  month = aug,
  number = {arXiv:1905.04899},
  eprint = {1905.04899},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.04899},
  urldate = {2025-06-16},
  abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/tororo.in/Zotero/storage/27L273QX/Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Classifiers with Localizable Features.pdf;/home/tororo.in/Zotero/storage/ZP5KGE9V/1905.html}
}

@inproceedings{yunCutMixRegularizationStrategy2019a,
  title = {{{CutMix}}: {{Regularization Strategy}} to {{Train Strong Classifiers With Localizable Features}}},
  shorttitle = {{{CutMix}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  year = {2019},
  pages = {6023--6032},
  urldate = {2025-06-16},
  file = {/home/tororo.in/Zotero/storage/6TPVL3J5/Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Classifiers With Localizable Features.pdf}
}

@inproceedings{zengIntentTunerInteractiveFramework2024,
  title = {{{IntentTuner}}: {{An Interactive Framework}} for {{Integrating Human Intentions}} in {{Fine-tuning Text-to-Image Generative Models}}},
  shorttitle = {{{IntentTuner}}},
  booktitle = {Proceedings of the 2024 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zeng, Xingchen and Gao, Ziyao and Ye, Yilin and Zeng, Wei},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--18},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642165},
  urldate = {2025-06-15},
  abstract = {Fine-tuning facilitates the adaptation of text-to-image generative models to novel concepts (e.g., styles and portraits), empowering users to forge creatively customized content. Recent efforts on fine-tuning focus on reducing training data and lightening computation overload but neglect alignment with user intentions, particularly in manual curation of multi-modal training data and intent-oriented evaluation. Informed by a formative study with fine-tuning practitioners for comprehending user intentions, we propose IntentTuner, an interactive framework that intelligently incorporates human intentions throughout each phase of the fine-tuning workflow. IntentTuner enables users to articulate training intentions with imagery exemplars and textual descriptions, automatically converting them into effective data augmentation strategies. Furthermore, IntentTuner introduces novel metrics to measure user intent alignment, allowing intent-aware monitoring and evaluation of model training. Application exemplars and user studies demonstrate that IntentTuner streamlines fine-tuning, reducing cognitive effort and yielding superior models compared to the common baseline tool.},
  isbn = {979-8-4007-0330-0},
  file = {/home/tororo.in/Zotero/storage/695FN9HG/Zeng et al. - 2024 - IntentTuner An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image.pdf}
}

@misc{zhangAddingConditionalControl2023,
  title = {Adding {{Conditional Control}} to {{Text-to-Image Diffusion Models}}},
  author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  year = {2023},
  month = nov,
  number = {arXiv:2302.05543},
  eprint = {2302.05543},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.05543},
  urldate = {2025-06-16},
  abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({$<$}50k) and large ({$>$}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Human-Computer Interaction,Computer Science - Multimedia},
  file = {/home/tororo.in/Zotero/storage/MTRFFEKQ/Zhang et al. - 2023 - Adding Conditional Control to Text-to-Image Diffusion Models.pdf;/home/tororo.in/Zotero/storage/TCNGLVYR/2302.html}
}

@inproceedings{zhangCommonSenseReasoning2025,
  title = {Common {{Sense Reasoning}} for~{{Deepfake Detection}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2024},
  author = {Zhang, Yue and Colman, Ben and Guo, Xiao and Shahriyari, Ali and Bharaj, Gaurav},
  editor = {Leonardis, Ale{\v s} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l},
  year = {2025},
  pages = {399--415},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-73223-2_22},
  abstract = {State-of-the-art deepfake detection approaches rely on image-based features extracted via neural networks. While these approaches trained in a supervised manner extract likely fake features, they may fall short in representing unnatural `non-physical' semantic facial attributes -- blurry hairlines, double eyebrows, rigid eye pupils, or unnatural skin shading. However, such facial attributes are easily perceived by humans and used to discern the authenticity of an image based on human common sense. Furthermore, image-based feature extraction methods that provide visual explanations via saliency maps can be hard to interpret for humans. To address these challenges, we frame deepfake detection as a Deepfake Detection VQA (DD-VQA) task and model human intuition by providing textual explanations that describe common sense reasons for labeling an image as real or fake. We introduce a new annotated dataset and propose a Vision and Language Transformer-based framework for the DD-VQA task. We also incorporate text and image-aware feature alignment formulation to enhance multi-modal representation learning. As a result, we improve upon existing deepfake detection models by integrating our learned vision representations, which reason over common sense knowledge from the DD-VQA task. We provide extensive empirical results demonstrating that our method enhances detection performance, generalization ability, and language-based interpretability in the deepfake detection task. Our dataset is available at https://github.com/Reality-Defender/Research-DD-VQA.},
  isbn = {978-3-031-73223-2},
  langid = {english},
  keywords = {Deepfake Detection,Vision and Language Model},
  file = {/home/tororo.in/Zotero/storage/H3JS37C8/Zhang et al. - 2025 - Common Sense Reasoning forÂ Deepfake Detection.pdf}
}

@misc{zhangMixupEmpiricalRisk2018,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and {Lopez-Paz}, David},
  year = {2018},
  month = apr,
  number = {arXiv:1710.09412},
  eprint = {1710.09412},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.09412},
  urldate = {2025-06-16},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tororo.in/Zotero/storage/TP77GKQT/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf}
}

@misc{zhangMixupEmpiricalRisk2018a,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and {Lopez-Paz}, David},
  year = {2018},
  month = apr,
  number = {arXiv:1710.09412},
  eprint = {1710.09412},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.09412},
  urldate = {2025-06-16},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tororo.in/Zotero/storage/S2XRDVCD/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf;/home/tororo.in/Zotero/storage/F3WNVWBC/1710.html}
}

@article{zhengBreakingSemanticArtifacts2024,
  title = {Breaking {{Semantic Artifacts}} for {{Generalized AI-generated Image Detection}}},
  author = {Zheng, Chende and Lin, Chenhao and Zhao, Zhengyu and Wang, Hang and Guo, Xu and Liu, Shuai and Shen, Chao},
  year = {2024},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {59570--59596},
  urldate = {2025-06-16},
  langid = {english},
  file = {/home/tororo.in/Zotero/storage/WZFTE737/Zheng et al. - 2024 - Breaking Semantic Artifacts for Generalized AI-generated Image Detection.pdf}
}

@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{karvonen_intuitive_2024,
	title = {An {Intuitive} {Explanation} of {Sparse} {Autoencoders} for {Mechanistic} {Interpretability} of {LLMs}},
	url = {https://www.lesswrong.com/posts/CJPqwXoFtgkKPRay8/an-intuitive-explanation-of-sparse-autoencoders-for},
	abstract = {Sparse Autoencoders (SAEs) have recently become popular for interpretability of machine learning models (although SAEs have been around since 1997).â¦},
	language = {en},
	urldate = {2025-06-16},
	author = {Karvonen, Adam},
	month = jun,
	year = {2024},
}

@article{kutsyk_sparse_2024,
	title = {Do {Sparse} {Autoencoders} ({SAEs}) transfer across base and finetuned language models?},
	url = {https://www.lesswrong.com/posts/bsXPTiAhhwt5nwBW3/do-sparse-autoencoders-saes-transfer-across-base-and},
	abstract = {This is a project submission post for the AI Safety Fundamentals course from BlueDot Impact. Therefore, some of its sections are intended to be beginâ¦},
	language = {en},
	urldate = {2025-06-16},
	author = {Kutsyk, Taras and Mencattini, Tommaso and Florea, Ciprian},
	month = sep,
	year = {2024},
}