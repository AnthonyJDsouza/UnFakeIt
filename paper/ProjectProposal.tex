% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}      % To produce the REVIEW version


% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{textcmds}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Team 81: Deepfake Detection}

\author{
Anthony John Dsouza\\
7053485\\
\and
Eva Morgan Toulouse Gavaller\\
Matriculation 2\\
%\and
%Name 3\\
%Matriculation 3\\
}
\maketitle

%%%%%%%%% BODY TEXT
\section{Task and Motivation}
% test \cite{zhangCommonSenseReasoning2025}
\subsection{Task statement \& motivation}
With recent advancements in AI, generative models like the transformer, diffusion, SSMs make use of vast \& diverse training data to generate text, images and audio indistinguishable from human generated data. These technologies have found use in Machine Translation, human machine interaction, image inpainting, text to video / image generation, etc. While many of these applications are beneficial to society, some bad actors use these tools for nefarious purposes, like spreading misinformation and revenge pornography. 

- would like to explore methods to detect deepfakes / ai generated media 
- collect ai generated visual dataset, annotate it for 


-- train VIT based model to detect deepfakes

-- train a VL model [PaliGemma, phi, etc] to reason with features extracted by image encoder and why provided image / video is real or fake

-- what features do classifiers / detectors focus on? [look at attention rollout, SAE features, any more techniques?]

\subsection{Motivation}
\ldots

\subsection{Related work}
Most common approaches use a simple setup, by fine-tuning a CNN / Vision Transformer \cite{dosovitskiyImageWorth16x162021} with some simple augmentations on datasets like \cite{dolhanskyDeepFakeDetectionChallenge2020, rosslerFaceForensicsLargescaleVideo2018, zhengBreakingSemanticArtifacts2024, liCelebDFLargeScaleChallenging2020}. The approach considered here is `classification', where the detector tries to identify artifacts generated by the generators. The DeepFake Detection challenge by Meta and Kaggle \footnote{\href{https://ai.meta.com/datasets/dfdc/}{DeepFake Detection Challenge}} saw many submissions using the above approaches with the best performing submission \cite{seferbekovDeepfakeDetectionChallenge} being an ensamble of many models with augmentations and more importantly face warping artifacts \cite{liExposingDeepFakeVideos2019}. Other approaches used \cite{yunCutMixRegularizationStrategy2019a, hendrycks*AugMixSimpleData2019, zhangMixupEmpiricalRisk2018a} for improved generalization.

In \cite{feiExposingAIgeneratedVideos2021}, the authors use motion magnification to amplify imperceptible movements in videos, making them observable. Initially developed to detect subtle vibrations in structures like high-rise buildings and bridges, this technique revealed that synthetic videos exhibit a higher flicker rate compared to real videos. To analyze these differences, the authors use InceptionV3 \cite{szegedyRethinkingInceptionArchitecture2015} as the backbone for extracting spatial features from video frames and an LSTM \cite{hochreiterLongShortTermMemory1997} to process temporal data, enabling effective discrimination between real and synthetic videos based on motion patterns.

In \cite{zhangCommonSenseReasoning2025}, authors reframe deepfake detection as a common sense reasoning task, aiming to address limitations of image classifiers that fail to identify subtle inconsistencies, such as unnatural skin tones or duplicate features like double eyebrows. Using a language model, the method reasons whether an image is real or fake, specifically highlighting inconsistencies in the temporal domain. This allows for a more nuanced analysis of visual content beyond standard detection techniques. For this, the authors use a BLIP \cite{liBLIPBootstrappingLanguageImage2022} backbone, integrating a BERT \cite{devlinBERTPretrainingDeep2019} like text encoder and a Vision Transformer \cite{dosovitskiyImageWorth16x162021} based image encoder, with cross-modal attention mechanism for vision language grounding between the two modalities. The textual outputs generated by the encoders are then fed into the language model that reasons if the provided image is real or fake.

SurFake's \cite{ciamarraDeepfakeDetectionExploiting2024} approach is based around concept that pixels in an image contain critical information about scene geometry and the acquisition process, which is often altered by deepfake generators, leaving detectable traces. By analyzing these pixel-level changes, SurFake aims to identify manipulations that traditional methods might overlook. The detection pipeline consists of three key steps- First, it performs face detection and extraction to isolate the relevant facial region. Next, a Global Surface Descriptor, based on UpRightNet \cite{xianUprightNetGeometryAwareCamera2019}, is used to capture the geometrical characteristics of the extracted face. Finally, these features are processed by a convolutional neural network (CNN), which analyzes the geometric data to determine whether the image has been manipulated, enabling robust deepfake detection.

\cite{huangSIDASocialMedia2025} created the Social Media Image Detection Dataset, comprising 300,000 images that include real, AI-generated, and tampered images. This dataset emphasizes broad diversity and realism to ensure it reflects the complexities of images encountered on social media platforms, providing a robust resource for developing and evaluating detection models. for detection, the authors develop on LISA \cite{laiLISAReasoningSegmentation2024} model by introducing two new tokens to its vocabulary: one for detection \texttt{<DET>} and one for segmentation \texttt{<SEG>}. The model takes an image and a prompt, such as “Can you identify if this image is real, fully synthetic, or tampered?” as input. It then generates a text description explaining its reasoning, while the last hidden layer contains the two additional tokens. The \texttt{<DET>} token identifies whether the image is manipulated, and if tampering is detected, the \texttt{<SEG>} token highlights the specific regions that have been altered, enabling precise localization of modifications.

\subsection{Challenges}
A significant portion of methods for DeepFake detection systems were built before diffusion \cite{hoDenoisingDiffusionProbabilistic2020} was a thing- mainly because of the fact that training these models was computationally expensive. Soon, \cite{dhariwalDiffusionModelsBeat2021} showed that diffusion based image generation models were better at image generation while being stable in comparison to their counterparts.\cite{rombachHighResolutionImageSynthesis2022}, \cite{lipmanFlowMatchingGenerative2023} build on this for improved generation, with methods developed to control the generation process like \cite{ruizDreamBoothFineTuning2023, zhangAddingConditionalControl2023} and using fewer steps for denoising as in \cite{lipmanFlowMatchingGenerative2023}. All these result in hyperrealistic image generation with few artifacts, making the process of detecting deepfakes highly difficult. 


\section{Goals}

\subsection{Challenges we aim to address}
\ldots

\subsection{Proposed Timeline}
\ldots 

\section{Methods}
\subsection{ViT based classifier}
\subsubsection{Stage 1: Training classifier}
In this stage, a ViT model \cite{dosovitskiyImageWorth16x162021} will be fine-tuned on the collected dataset so that the model is able to learn features necessary for classifying between real and AI generated images. The reason a vision transformer is chosen over CNNs is because the former learns robust representations from the data.

\subsubsection{Stage 2: Interpretability - SAE and Attention rollout}
Sparse autoencoders are small models that are trained on hidden representations for a model with an aim to sparsify entangled features in the hidden representations of large models. This makes models more interpretable, essentially helping in deriving human understandable explanations for behaviours of the model. The SAE will be trained by harvesting hidden representations from the model and help us understand what features the model looks at to classify between real and AI generated images.

The main challenge in interpretability is polysemanticity, the notion that neurons in a neural antework can activate from multiple, unrelated inputs, likely due to a model using neurons efficiently, in order to leave other neurons available for important tasks \cite{olah2020zoom}. Polysemanticity is attributed to superposition, where models represent more concepts than available neurons, leading to concepts being represented by various neurons at once  \cite{karvonen_intuitive_2024}.  This in turn makes interpreting specific model features difficult, which is why we use SAEs. SAEs create a matrix of sparse, more interpretable, encoded representations from LLM input  \cite{karvonen_intuitive_2024}. Encoded representations can also be called features. These features may activate based on different inputs, and through these activations, we can better interpret what information these features activate on. Additionally, as SAEs are given the incentive to create sparse vectors as a result of imposing L1 loss, we get only a few non-zero values, which are theoretically the \qq{true} features of the model \cite{kutsyk_sparse_2024}. 

Attention rollout \cite{abnarQuantifyingAttentionFlow2020} is another method to make later attention layers more interpretable.

\section{Datasets}
For this project, we intend to collect publicly available AI generated images / videos from social media sites like Facebook, Reddit, X, Instagram, etc. After collection, we plan to annotate them. This is because there is no publicly available dataset that is regularly updated to keep up with new generators like StableDiffusion, FLUX, etc. 

\section{Evaluation}

\subsection{ViT based model}
\begin{equation}
\text{LogLoss} = - \frac{1}{N} \sum_{i=1}^{N}[y_{i}\log(p_i) + (1 - y_{i}) \log(1 - p_i)]
\label{eq:logloss}
\end{equation}
For evaluating the ViT based model, our metrics will be Accuracy and LogLoss. The reason for the latter is that LogLoss penalizes classifications that are confidently wrong i.e. the model will be penalized when it assigns a low probability to a true event. Another factor we consider is that LogLoss is also better suited for situations where confidence of prediction matters, as it looks at probabilities of classes.

\subsection{Vision-Language model}
In this case, we consider the LogLoss as in \ref{eq:logloss} for classification and also inter-annotator agreement for the reasons generated by the LM backbone. The latter also provides us insight on what features the model reasons are important to observe. 
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

\end{document}
